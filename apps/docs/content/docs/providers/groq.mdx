---
title: Groq
---

AISDK provides first-class support for Groq with fully typed model APIs.
Model capabilities are enforced at compile time using Rust's type system.
This prevents model capability mismatches and guarantees the selected model is valid for the task (e.g. tool calling).


## Installation

Enable the Groq provider feature:

```bash
cargo add aisdk --features groq
```

This installs AISDK with the Groq provider enabled. Once you have enabled the Groq provider, you can use all aisdk <Link href="/docs#core-features">features</Link> with it.

## Create a Provider Instance

To create a provider instance, call `Groq::model_name()`, where **model_name** is the Groq model you want to use.
Model names are exposed as snake-case methods.

```rust
use aisdk::providers::Groq;

let groq = Groq::llama_3_1_8b_instruct();
```

This initializes the provider with:

* Model: `"llama-3.1-8b-instruct"`
* API key from environment (if set with `GROQ_API_KEY`)
* Groq's default base URL (https://api.groq.com/openai/)

## Basic Text Generation

Example using [LanguageModelRequest](https://docs.rs/aisdk/latest/aisdk/core/struct.LanguageModelRequest.html) for text generation.

```rust
use aisdk::{
    core::LanguageModelRequest,
    providers::Groq,
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {

    let groq = Groq::llama_3_1_8b_instruct();

    let response = LanguageModelRequest::builder()
        .model(groq)
        .prompt("Write a short poem about Rust.")
        .build()
        .generate_text()
        .await?;

    println!("Response text: {:?}", response.text());
    Ok(())
}
```

## Provider Settings

You can customize provider configuration using `Groq::builder()`

### API Key

```rust
let groq = Groq::<MetaLlamaLlama318bInstructV10>::builder()
    .api_key("your-api-key")
    .build()?;
```

If not specified, AISDK uses the `GROQ_API_KEY` environment variable.

### Base URL

Useful when routing through a proxy, gateway, or self-hosted compatible endpoint.

```rust
let groq = Groq::<MetaLlamaLlama318bInstructV10>::builder()
    .base_url("https://api.groq.com/openai/")
    .build()?;
```

### Provider Name

For logging, analytics, and observability.

```rust
let groq = Groq::<MetaLlamaLlama318bInstructV10>::builder()
    .provider_name("Groq")
    .build()?;
```

### Full Custom Configuration Example

```rust
let groq = Groq::<MetaLlamaLlama318bInstructV10>::builder()
    .api_key("your-api-key")
    .base_url("https://api.groq.com/openai/")
    .provider_name("Groq")
    .build()?;
```

## Dynamic Model Selection

For runtime model selection (e.g., loading models from config files), use `DynamicModel`:

### Using model_name() Method with Default Settings

```rust
use aisdk::providers::Groq;

// Specify model as a string at runtime
let groq = Groq::model_name("llama-3.1-8b-instruct");
```

### Using Builder Pattern with Custom Settings

```rust
use aisdk::{
    core::DynamicModel,
    providers::Groq,
};

let groq = Groq::<DynamicModel>::builder()
    .model_name("llama-3.1-8b-instruct")
    .api_key("your-api-key")
		.base_url("https://api.groq.com/openai/")
    .build()?;
```

<Callout type="warn">
**Warning**: When using `DynamicModel`, model capabilities are **not validated at compile time**. 
This means there's no guarantee the model supports requested features (e.g., tool calls, structured output).
For compile-time safety, use the typed methods like `Groq::llama_3_1_8b_instruct()`.
</Callout>

## Next Steps

* Take a deeper look at text generation features [Generating Text](/docs/concepts/language-model-request#generate_text) / [Streaming Text](/docs/concepts/language-model-request#stream_text)
* Explore [Structured Output](/docs/concepts/structured-output) for reliable agent data.
* Learn how to create [Custom Tools](/docs/concepts/tools).
* Learn more about [Agents](/docs/concepts/agents).
